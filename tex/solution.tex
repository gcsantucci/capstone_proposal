
\section{Solution Statement}

As describe in section \ref{Data}, we can reduce the amount of background events by a factor of 100 already, keeping only the interesting neutrino events that can be mistaken as a proton decay signal increasing the false positive rate. But the number of background events remaining is still high and need to be reduced event further\footnote{Sections \ref{Benchmark} and \ref{Metric} describe why it is so important to have such a small number of false positives in this type of search.}.

Our goal here is to use machine learning algorithms to learn an optimized way of classifying events as signal or background. The way the standard analysis is done is by using a decision tree, since it simply applies boundary cuts on different features of the data. A natural choice of algorithm for the new analysis could be Boosted Decision Trees to continue in the same direction. Because of systematic uncertainties that have to be taken into account, dealing with boundary cuts can be a lot easier that dealing with complicated decisions, like the output of neural nets.

But since this approach is completely new, there is no reason to be attached to the standard way and new algorithms can be tested. The plan is to test different algorithms like for example Random Forest and Support Vector Machines. Both these algorithms can find a new boundary that separates both classes in feature space.
Applying a dimensionality reduction algorithm is something typical in ML applications, but it will not be used here. There are two main reasons for that, one is that our feature space is not that big compared to image recognition for instance, where the number of features is thousands of times bigger than ours. Also, we want to keep track of the features that are being used in the feature space itself, since they have a one-to-one mapping with physical quantities. If an algorithm like Principal Component Analysis is applied, the new space is a linear combination of the original features which complicates the interpretation of final results and dealing with systematic errors. The treatment of systematic uncertainties is beyond the purpose of this study, but it is necessary to keep it in mind when choosing an algorithm for the analysis.

Here we simply want find an algorithm with simple interpretation that performs well given some metric that will be discussed in Section \ref{Metric}. The goal is to perform better than the standard analysis and there are two ways this can be achieved. Either we improve the efficiency of keeping signal events while having the same amount of background events. Or we keep a similar efficiency while reducing the number of background by a factor of 2 or so. Of course, achieving both higher efficiency and reduced number of background is the best possible scenario. But if only one of them is achieved the effort can be considered a success. The definition of efficiency that will be used in this work is given by eq. \ref{eq:eff} in Section \ref{Benchmark}.
